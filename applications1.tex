\section{Application 1: RATING SCALES FOR COLLECTIVE INTELLIGENCE IN
INNOVATION COMMUNITIES}

In~\cite{ratings}, the authors studied one of the applications of collective intelligence: open innovation and crowdsourcing for commercial ideas. In particular, they focused on investigating how different rating scales would impact the process of identifying the best ideas in collective decision makings involving many individual evaluators in online innovation communities. 

In their study, the authors ~\cite{ratings} used a set of 24 real-world ideas in a public idea competition ~\cite{blohm2011does}. Their study included three parts: a web-based experiment, a survey to measure the rating satisfaction of the participating evaluators, and the rating of ideal quality by 7 independent experts. The three rating scales that they compared were: binary rating including thumbs-up and thumbs-down, 5-star rating, and complex rating which involved four 5-star scales to reflect the four dimensions of idea quality ~\cite{blohm2011does}: novelty, feasibility, strategic relevance and elaboration. 

In ~\cite{ratings}, the authors assumed that the cognitive process of rating ideas in open innovation platforms is similar to the process of responding to a survey and thus the decision process for a community member to rate an idea involves 4 basic steps ~\cite{tourangeau2000psychology}: 

1) Comprehension

In this step, the evaluators read the idea and assess its form and meaning.

2) Information Retrieval

In this step, the evaluators recall relevant information from long-term memory and bringin it into an active state, so that it can be used to rate the quality of the ideas. They usually need to use cues to trigger the recall of information, remember generic and specific
memories and fill in missing details through inference.

3) Judgment

In this step, the evaluators formulate their answer to the idea rating task based on evaluations of the retrieved information.

4) Reporting and Response Selection

This is the last step in which the evaluators map their judgment onto the given
response options and turn their judgments into close-ended items with an ordered set of
response categories.

Based on prior research and analysis, the authors proposed four hypotheses to test for their study regarding the granularity of the rating scale, rating accuracy, users' satisfaction with their ratings and levels of user expertise:

\emph{H1}: Rating accuracy is positively influenced by the granularity of the rating scale.

\emph{H2}: Users' satisfaction with their ratings is positively influenced by the granularity of the rating scale.

\emph{H3}: The positive relationship between the rating scale granularity and rating accuracy will be weakened for high levels of user expertise and
strengthened for low levels of user expertise.

\emph{H4}: The positive relationship between between rating scle granularity and rating satisfaction will be strengthened for high levels of user expertise and
weakened for low levels of user expertise.

The theoretical consideration for \emph{H3} is that it would be easier for more expertised raters to integrate the different aspects of
their decision on a given rating scale than for less knowledgeable users, thus they might be better at expressing their quality judgments on a binary, thumbs up/down scale than less experienced ones.

Similarly, the theoretical consideration for \emph{H4} is that raters having a low expertise are likely to be less confident less satisfied with their rating results thus they may fell a greater sense of insecurity when facing a more complex rating scles than more knowledgeable evaluators.

In their experiment ~\cite{ratings}, 313 idea raters submitted 15,864 ratings in total. After the experiment, user expertise and rating satisfaction information were collected with an online survey. Parallelly, these ideas were rated by a qualifies expert jury to assess the validity of the idea quality.

As expected, the results showed that the granularity of the rating scale would positively influence rating
accuracy (\emph{H1}) and positively influence usersâ€™ satisfaction with their rating accuracy (\emph{H2}). In fact, the complex rating scale leads to a
significantly higher rating accuracy than the binary rating and the 5-star rating schemes. The commonly used promote/demote rating scale may suffer from user bias of either rating positively or negatively and rating scales with higher granularity offer evaluators more discretion for mapping their judgement, which lead to higher rating accuracy.

However, the results failed to support the hypotheses \emph{H3} and \emph{H4} regarding the moderating influence of user expertise. The author tried to explain this with the \lq wisdom of crowd \rq theory which argues that a larger group of people can perform decision tasks as good
as experts irrespective of the knowledge of the individual and a \lq crowd \rq can perform similar to experts. Based on this analysis, the authors believed that the \lq best \rq rating scheme in their study, the complex rating scale, could achieve best performance for all user groups.




